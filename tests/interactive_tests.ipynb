{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cdf13b4-88bf-4443-b8f2-0eb74d65e0cb",
   "metadata": {},
   "source": [
    "Pour le moment, pour mener les tests comme si c'était un package installé, export path dans le .bashrc ou sys.path.append dans les script pour importer plug_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280eb6b-be5e-40d9-a95c-ce6c2a7a6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('/gpfswork/rech/ibu/ssos023/Plug-AI')\n",
    "#!export PYTHONPATH=$PYTHONPATH:/gpfsdswork/projects/rech/ibu/ssos023/Plug-AI/plug_ai\n",
    "#!export PYTHONPATH=$PYTHONPATH:/gpfsdswork/projects/rech/ibu/ssos023/Plug-AI/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf1ae1-9c79-4ecf-b6c6-72baec1ad039",
   "metadata": {},
   "source": [
    "        print(config)WARNING: There is an issue on frontal nodes. Kernel dies and there is a cuda initialization (from where? apparently dynunet from monai.networks.nets import DynUNet) that fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e812d-3ce9-4c08-9da7-2ae5a33dd622",
   "metadata": {},
   "source": [
    "### Test cells (to be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df34219-1a35-40b8-ab51-96735fb301ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe should we complete arguments list in cli automatically to include the dataset & model arguments\n",
    "# We can retrieve args (and types if defined) and add a fake init just like for managers to each dataset/model we define and have the manager inherit those args\n",
    "# For now, we add manually  (up to the user) those args in dataset_kwargs\n",
    "#monai.apps.DecathlonDataset.__init__.__code__.co_varnames\n",
    "#inspect.getfullargspec(monai.apps.DecathlonDataset.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8af70-2cc2-4137-8f2a-c691d5a9f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#monai.apps.MedNISTDataset(root_dir = \"/gpfswork/rech/ibu/commun/datasets\", \n",
    "#                  section = \"training\",\n",
    "#                  download=download)\n",
    "MedNIST(dataset_dir = , download_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e04a8-e34c-4901-b787-448f0e92a0aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test CLI APP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cef5c5-d61c-46e2-a9aa-b48edb49dff0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CLI run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d9198-841b-4f02-87e4-dc0bfa0ed26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfsdswork/projects/rech/ibu/ssos023/Plug-AI/config_exemples/config_nnUNet.yaml\n",
      "Plug-AI running with config:\n",
      "\t dataset : nnU-Net\n",
      "\t dataset_kwargs : {'nnunet_dataset_rootdir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_raw_data_base', 'task_id': 4, 'raw_dataset_type': 'MSD', 'raw_dataset_dir': '/gpfswork/rech/ibu/commun/datasets/MSD/Task04_Hippocampus', 'download_raw_dataset': False, 'nbr_thread': 4, 'progress': True}\n",
      "\t preprocess : False\n",
      "\t preprocess_kwargs : None\n",
      "\t generate_signature : True\n",
      "\t train_ratio : 0.8\n",
      "\t val_ratio : 0\n",
      "\t limit_sample : 10\n",
      "\t batch_size : 2\n",
      "\t shuffle : True\n",
      "\t drop_last : True\n",
      "\t model : nnU-Net\n",
      "\t model_kwargs : {'nnunet_dataset_rootdir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_raw_data_base', 'nnunet_preprocessed_rootdir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_preprocessed', 'task_id': 4, 'verify_dataset_integrity': False, 'no_preprocessing': False, 'planner_2d': 'ExperimentPlanner2D_v21', 'planner_3d': 'ExperimentPlanner3D_v21', 'nbr_thread': 8, 'overwrite_plans': None, 'overwrite_plans_identifier': None, 'progress': True}\n",
      "\t nb_epoch : 1\n",
      "\t learning_rate : 5e-05\n",
      "\t device : cuda\n",
      "\t random_seed : 2022\n",
      "\t report_log : False\n",
      "\t loop : nnU-Net\n",
      "\t loop_kwargs : {'nnunet_dataset_rootdir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_raw_data_base', 'nnunet_preprocessed_rootdir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_preprocessed', 'nnunet_experiment_dir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_preprocessed', 'network': '2d', 'network_trainer': 'nnUNetTrainerV2', 'task': 4, 'fold': 'all'}\n",
      "\t optimizer : SGD\n",
      "\t optimizer_kwargs : {'lr': 0.0001, 'momentum': 0.99, 'weight_decay': 3e-05, 'nesterov': True}\n",
      "\t criterion : DiceCELoss\n",
      "\t criterion_kwargs : {'to_onehot_y': True, 'softmax': True}\n",
      "\t config_file : ../config_exemples/config_nnUNet.yaml\n",
      "\t export_config : None\n",
      "\t mode : TRAINING\n",
      "\t verbose : FULL\n",
      "\t seed : None\n",
      "==================================== Dataset initialization ... ====================================\n",
      "Running with interpreted config:\n",
      "\t {'dataset': 'nnU-Net', 'dataset_kwargs': {'nnunet_dataset_rootdir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_raw_data_base', 'task_id': 4, 'raw_dataset_type': 'MSD', 'raw_dataset_dir': '/gpfswork/rech/ibu/commun/datasets/MSD/Task04_Hippocampus', 'download_raw_dataset': False, 'nbr_thread': 4, 'progress': True}, 'preprocess': 'False', 'preprocess_kwargs': {}, 'mode': 'TRAINING', 'batch_size': 2, 'train_ratio': 0.8, 'val_ratio': 0.0, 'limit_sample': 10, 'shuffle': True, 'drop_last': True, 'seed': None, 'verbose': 'FULL'}\n",
      "Preprocessing the dataset\n",
      "Dataset type is valid\n",
      "nnunet_dict :  {'nnunet_dataset_rootdir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_raw_data_base', 'task_id': 4, 'raw_dataset_type': 'MSD', 'raw_dataset_dir': '/gpfswork/rech/ibu/commun/datasets/MSD/Task04_Hippocampus', 'download_raw_dataset': False, 'nbr_thread': 4, 'progress': True}\n",
      "Now converting MSD to nnUnet\n",
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "nnU-Net\n",
      "===================================== Model initialization ... =====================================\n",
      "Running with interpreted config:\n",
      "\t {'model': 'nnU-Net', 'model_kwargs': {'nnunet_dataset_rootdir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_raw_data_base', 'nnunet_preprocessed_rootdir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_preprocessed', 'task_id': 4, 'verify_dataset_integrity': False, 'no_preprocessing': False, 'planner_2d': 'ExperimentPlanner2D_v21', 'planner_3d': 'ExperimentPlanner3D_v21', 'nbr_thread': 8, 'overwrite_plans': None, 'overwrite_plans_identifier': None, 'progress': True}, 'device': 'cuda', 'mode': 'TRAINING', 'verbose': 'FULL'}\n",
      "Model type is valid\n",
      "Model preparation done!\n",
      "=================================== Execution initialization ... ===================================\n",
      "Running with interpreted config:\n",
      "\t {'loop': <class 'plug_ai.runners.nnUNet.nnUNet_Trainer'>, 'loop_kwargs': {'nnunet_dataset_rootdir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_raw_data_base', 'nnunet_preprocessed_rootdir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_preprocessed', 'nnunet_experiment_dir': '/gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_preprocessed', 'network': '2d', 'network_trainer': 'nnUNetTrainerV2', 'task': 4, 'fold': 'all'}, 'mode': 'TRAINING', 'nb_epoch': 1, 'learning_rate': 5e-05, 'device': 'cuda', 'seed': None, 'report_log': False, 'criterion': <class 'monai.losses.dice.DiceCELoss'>, 'criterion_kwargs': {'to_onehot_y': True, 'softmax': True}, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'optimizer_kwargs': {'lr': 0.0001, 'momentum': 0.99, 'weight_decay': 3e-05, 'nesterov': True}, 'verbose': 'FULL', 'dataset_manager': <plug_ai.managers.managers.DatasetManager object at 0x146407b6f6a0>, 'model_manager': <plug_ai.managers.managers.ModelManager object at 0x146515b1b3a0>}\n",
      "TRAINING MODE : \n",
      "dict_keys(['nnunet_preprocessed_rootdir', 'network', 'nnunet_experiment_dir', 'fold', 'task', 'network_trainer', 'nnunet_dataset_rootdir'])\n",
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n",
      "###############################################\n",
      "I am running the following nnUNet: 2d\n",
      "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>\n",
      "For that I will be using the following configuration:\n",
      "num_classes:  2\n",
      "modalities:  {0: 'MRI'}\n",
      "use_mask_for_norm OrderedDict([(0, False)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes OrderedDict([(0, 'nonCT')])\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 366, 'num_pool_per_axis': [3, 3], 'patch_size': array([56, 40]), 'median_patient_size_in_voxels': array([36, 50, 35]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}\n",
      "\n",
      "I am using stage 0 from these plans\n",
      "I am using batch dice + CE loss\n",
      "\n",
      "I am using data from this folder:  /gpfswork/rech/ibu/commun/nnUNet_experiment_kg/nnUNet_preprocessed/Task004_Hippocampus/nnUNetData_plans_v2.1_2D\n",
      "###############################################\n",
      "loading dataset\n",
      "loading all case properties\n",
      "unpacking dataset\n",
      "done\n",
      "2023-01-04 18:24:15.642464: lr: 0.01\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2023-01-04 18:24:20.727968: Unable to plot network architecture:\n",
      "2023-01-04 18:24:20.735009: No module named 'hiddenlayer'\n",
      "2023-01-04 18:24:20.735145: \n",
      "printing the network instead:\n",
      "\n",
      "2023-01-04 18:24:20.735257: Generic_UNet(\n",
      "  (conv_blocks_localization): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (instnorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (instnorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (instnorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (instnorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (instnorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (instnorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_blocks_context): ModuleList(\n",
      "    (0): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (instnorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (instnorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (instnorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (instnorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedConvLayers(\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvDropoutNormNonlin(\n",
      "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "          (instnorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "        (1): ConvDropoutNormNonlin(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (instnorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "            (instnorm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): StackedConvLayers(\n",
      "        (blocks): Sequential(\n",
      "          (0): ConvDropoutNormNonlin(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (instnorm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (td): ModuleList()\n",
      "  (tu): ModuleList(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "    (1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "    (2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "  )\n",
      "  (seg_outputs): ModuleList(\n",
      "    (0): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "2023-01-04 18:24:20.736954: \n",
      "\n",
      "2023-01-04 18:24:20.737126: \n",
      "epoch:  0\n",
      "2023-01-04 18:24:55.192172: train loss : -0.0622\n",
      "2023-01-04 18:25:01.104553: validation loss: -0.3591\n",
      "2023-01-04 18:25:01.105136: Average global foreground Dice: [0.3275, 0.5909]\n",
      "2023-01-04 18:25:01.107935: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:25:01.885515: lr: 0.009991\n",
      "2023-01-04 18:25:01.885758: This epoch took 41.148524 s\n",
      "\n",
      "2023-01-04 18:25:01.885867: \n",
      "epoch:  1\n",
      "2023-01-04 18:25:32.959903: train loss : -0.5553\n",
      "2023-01-04 18:25:38.938425: validation loss: -0.7538\n",
      "2023-01-04 18:25:38.938970: Average global foreground Dice: [0.8411, 0.8189]\n",
      "2023-01-04 18:25:38.939116: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:25:39.576738: lr: 0.009982\n",
      "2023-01-04 18:25:39.580721: saving checkpoint...\n",
      "2023-01-04 18:25:39.628209: done, saving took 0.05 seconds\n",
      "2023-01-04 18:25:39.631541: This epoch took 37.745571 s\n",
      "\n",
      "2023-01-04 18:25:39.631686: \n",
      "epoch:  2\n",
      "2023-01-04 18:26:11.171394: train loss : -0.7427\n",
      "2023-01-04 18:26:17.462534: validation loss: -0.7938\n",
      "2023-01-04 18:26:17.463061: Average global foreground Dice: [0.8641, 0.8466]\n",
      "2023-01-04 18:26:17.463214: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:26:18.129469: lr: 0.009973\n",
      "2023-01-04 18:26:18.133369: saving checkpoint...\n",
      "2023-01-04 18:26:18.163064: done, saving took 0.03 seconds\n",
      "2023-01-04 18:26:18.168388: This epoch took 38.536586 s\n",
      "\n",
      "2023-01-04 18:26:18.168532: \n",
      "epoch:  3\n",
      "2023-01-04 18:26:49.615239: train loss : -0.7723\n",
      "2023-01-04 18:26:55.973127: validation loss: -0.8079\n",
      "2023-01-04 18:26:55.973682: Average global foreground Dice: [0.8732, 0.8567]\n",
      "2023-01-04 18:26:55.973831: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:26:56.646869: lr: 0.009964\n",
      "2023-01-04 18:26:56.650865: saving checkpoint...\n",
      "2023-01-04 18:26:56.683531: done, saving took 0.04 seconds\n",
      "2023-01-04 18:26:56.690343: This epoch took 38.521703 s\n",
      "\n",
      "2023-01-04 18:26:56.690500: \n",
      "epoch:  4\n",
      "2023-01-04 18:27:27.621498: train loss : -0.7885\n",
      "2023-01-04 18:27:33.717065: validation loss: -0.8176\n",
      "2023-01-04 18:27:33.717630: Average global foreground Dice: [0.8803, 0.8635]\n",
      "2023-01-04 18:27:33.717803: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:27:34.365324: lr: 0.009955\n",
      "2023-01-04 18:27:34.369285: saving checkpoint...\n",
      "2023-01-04 18:27:34.400950: done, saving took 0.04 seconds\n",
      "2023-01-04 18:27:34.405386: This epoch took 37.714783 s\n",
      "\n",
      "2023-01-04 18:27:34.405616: \n",
      "epoch:  5\n",
      "2023-01-04 18:28:05.797687: train loss : -0.7975\n",
      "2023-01-04 18:28:11.745225: validation loss: -0.8230\n",
      "2023-01-04 18:28:11.745809: Average global foreground Dice: [0.8828, 0.8684]\n",
      "2023-01-04 18:28:11.746028: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:28:12.371271: lr: 0.009946\n",
      "2023-01-04 18:28:12.390158: saving checkpoint...\n",
      "2023-01-04 18:28:12.434484: done, saving took 0.06 seconds\n",
      "2023-01-04 18:28:12.449917: This epoch took 38.044194 s\n",
      "\n",
      "2023-01-04 18:28:12.450086: \n",
      "epoch:  6\n",
      "2023-01-04 18:28:43.462266: train loss : -0.8050\n",
      "2023-01-04 18:28:49.726498: validation loss: -0.8277\n",
      "2023-01-04 18:28:49.727064: Average global foreground Dice: [0.8863, 0.8713]\n",
      "2023-01-04 18:28:49.727273: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:28:50.370255: lr: 0.009937\n",
      "2023-01-04 18:28:50.389119: saving checkpoint...\n",
      "2023-01-04 18:28:50.428150: done, saving took 0.06 seconds\n",
      "2023-01-04 18:28:50.432783: This epoch took 37.982593 s\n",
      "\n",
      "2023-01-04 18:28:50.432953: \n",
      "epoch:  7\n",
      "2023-01-04 18:29:21.621922: train loss : -0.8100\n",
      "2023-01-04 18:29:27.948565: validation loss: -0.8320\n",
      "2023-01-04 18:29:27.949142: Average global foreground Dice: [0.8892, 0.8741]\n",
      "2023-01-04 18:29:27.949293: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:29:28.577375: lr: 0.009928\n",
      "2023-01-04 18:29:28.581459: saving checkpoint...\n",
      "2023-01-04 18:29:28.609185: done, saving took 0.03 seconds\n",
      "2023-01-04 18:29:28.613809: This epoch took 38.180747 s\n",
      "\n",
      "2023-01-04 18:29:28.613957: \n",
      "epoch:  8\n",
      "2023-01-04 18:29:59.526639: train loss : -0.8140\n",
      "2023-01-04 18:30:05.782941: validation loss: -0.8353\n",
      "2023-01-04 18:30:05.783522: Average global foreground Dice: [0.8919, 0.8767]\n",
      "2023-01-04 18:30:05.783672: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:30:06.498878: lr: 0.009919\n",
      "2023-01-04 18:30:06.502848: saving checkpoint...\n",
      "2023-01-04 18:30:06.531084: done, saving took 0.03 seconds\n",
      "2023-01-04 18:30:06.535645: This epoch took 37.921578 s\n",
      "\n",
      "2023-01-04 18:30:06.535788: \n",
      "epoch:  9\n",
      "2023-01-04 18:30:37.500837: train loss : -0.8172\n",
      "2023-01-04 18:30:43.673295: validation loss: -0.8377\n",
      "2023-01-04 18:30:43.673853: Average global foreground Dice: [0.8923, 0.878]\n",
      "2023-01-04 18:30:43.674002: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:30:44.315988: lr: 0.00991\n",
      "2023-01-04 18:30:44.319951: saving checkpoint...\n",
      "2023-01-04 18:30:44.349432: done, saving took 0.03 seconds\n",
      "2023-01-04 18:30:44.360591: This epoch took 37.824688 s\n",
      "\n",
      "2023-01-04 18:30:44.360982: \n",
      "epoch:  10\n",
      "2023-01-04 18:31:15.951286: train loss : -0.8209\n",
      "2023-01-04 18:31:22.422092: validation loss: -0.8411\n",
      "2023-01-04 18:31:22.422649: Average global foreground Dice: [0.895, 0.8804]\n",
      "2023-01-04 18:31:22.422796: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:31:23.084409: lr: 0.009901\n",
      "2023-01-04 18:31:23.088507: saving checkpoint...\n",
      "2023-01-04 18:31:23.114143: done, saving took 0.03 seconds\n",
      "2023-01-04 18:31:23.118976: This epoch took 38.757792 s\n",
      "\n",
      "2023-01-04 18:31:23.119135: \n",
      "epoch:  11\n",
      "2023-01-04 18:31:54.897703: train loss : -0.8231\n",
      "2023-01-04 18:32:00.780942: validation loss: -0.8437\n",
      "2023-01-04 18:32:00.781529: Average global foreground Dice: [0.8967, 0.8829]\n",
      "2023-01-04 18:32:00.781694: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:32:01.406879: lr: 0.009892\n",
      "2023-01-04 18:32:01.410860: saving checkpoint...\n",
      "2023-01-04 18:32:01.449159: done, saving took 0.04 seconds\n",
      "2023-01-04 18:32:01.453661: This epoch took 38.334410 s\n",
      "\n",
      "2023-01-04 18:32:01.453816: \n",
      "epoch:  12\n",
      "2023-01-04 18:32:32.550565: train loss : -0.8255\n",
      "2023-01-04 18:32:39.131326: validation loss: -0.8444\n",
      "2023-01-04 18:32:39.131908: Average global foreground Dice: [0.8966, 0.8839]\n",
      "2023-01-04 18:32:39.132068: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:32:39.761135: lr: 0.009883\n",
      "2023-01-04 18:32:39.765276: saving checkpoint...\n",
      "2023-01-04 18:32:39.794682: done, saving took 0.03 seconds\n",
      "2023-01-04 18:32:39.803923: This epoch took 38.349990 s\n",
      "\n",
      "2023-01-04 18:32:39.804168: \n",
      "epoch:  13\n",
      "2023-01-04 18:33:10.870255: train loss : -0.8275\n",
      "2023-01-04 18:33:17.453576: validation loss: -0.8462\n",
      "2023-01-04 18:33:17.454119: Average global foreground Dice: [0.8988, 0.8851]\n",
      "2023-01-04 18:33:17.454267: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:33:18.094825: lr: 0.009874\n",
      "2023-01-04 18:33:18.098786: saving checkpoint...\n",
      "2023-01-04 18:33:18.128282: done, saving took 0.03 seconds\n",
      "2023-01-04 18:33:18.132585: This epoch took 38.328283 s\n",
      "\n",
      "2023-01-04 18:33:18.132734: \n",
      "epoch:  14\n",
      "2023-01-04 18:33:49.209909: train loss : -0.8288\n",
      "2023-01-04 18:33:55.571121: validation loss: -0.8486\n",
      "2023-01-04 18:33:55.571687: Average global foreground Dice: [0.9005, 0.8859]\n",
      "2023-01-04 18:33:55.571852: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:33:56.313056: lr: 0.009865\n",
      "2023-01-04 18:33:56.317175: saving checkpoint...\n",
      "2023-01-04 18:33:56.345848: done, saving took 0.03 seconds\n",
      "2023-01-04 18:33:56.362763: This epoch took 38.229922 s\n",
      "\n",
      "2023-01-04 18:33:56.362994: \n",
      "epoch:  15\n",
      "2023-01-04 18:34:27.201487: train loss : -0.8312\n",
      "2023-01-04 18:34:33.431073: validation loss: -0.8488\n",
      "2023-01-04 18:34:33.431630: Average global foreground Dice: [0.8995, 0.8875]\n",
      "2023-01-04 18:34:33.431777: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:34:34.079246: lr: 0.009856\n",
      "2023-01-04 18:34:34.083245: saving checkpoint...\n",
      "2023-01-04 18:34:34.112730: done, saving took 0.03 seconds\n",
      "2023-01-04 18:34:34.117963: This epoch took 37.754856 s\n",
      "\n",
      "2023-01-04 18:34:34.118106: \n",
      "epoch:  16\n",
      "2023-01-04 18:35:04.897422: train loss : -0.8334\n",
      "2023-01-04 18:35:11.464338: validation loss: -0.8525\n",
      "2023-01-04 18:35:11.464936: Average global foreground Dice: [0.9022, 0.8909]\n",
      "2023-01-04 18:35:11.465084: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:35:12.093648: lr: 0.009847\n",
      "2023-01-04 18:35:12.097622: saving checkpoint...\n",
      "2023-01-04 18:35:12.128068: done, saving took 0.03 seconds\n",
      "2023-01-04 18:35:12.133384: This epoch took 38.015169 s\n",
      "\n",
      "2023-01-04 18:35:12.133531: \n",
      "epoch:  17\n",
      "2023-01-04 18:35:43.308573: train loss : -0.8347\n",
      "2023-01-04 18:35:49.510876: validation loss: -0.8537\n",
      "2023-01-04 18:35:49.511427: Average global foreground Dice: [0.9037, 0.8903]\n",
      "2023-01-04 18:35:49.511589: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:35:50.248710: lr: 0.009838\n",
      "2023-01-04 18:35:50.252699: saving checkpoint...\n",
      "2023-01-04 18:35:50.277944: done, saving took 0.03 seconds\n",
      "2023-01-04 18:35:50.296760: This epoch took 38.163124 s\n",
      "\n",
      "2023-01-04 18:35:50.296992: \n",
      "epoch:  18\n",
      "2023-01-04 18:36:20.959577: train loss : -0.8360\n",
      "2023-01-04 18:36:27.156619: validation loss: -0.8555\n",
      "2023-01-04 18:36:27.157166: Average global foreground Dice: [0.9048, 0.8918]\n",
      "2023-01-04 18:36:27.157320: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:36:27.784425: lr: 0.009829\n",
      "2023-01-04 18:36:27.788386: saving checkpoint...\n",
      "2023-01-04 18:36:27.818271: done, saving took 0.03 seconds\n",
      "2023-01-04 18:36:27.823751: This epoch took 37.526643 s\n",
      "\n",
      "2023-01-04 18:36:27.823915: \n",
      "epoch:  19\n",
      "2023-01-04 18:36:58.813116: train loss : -0.8367\n",
      "2023-01-04 18:37:05.106414: validation loss: -0.8561\n",
      "2023-01-04 18:37:05.106966: Average global foreground Dice: [0.9055, 0.8921]\n",
      "2023-01-04 18:37:05.107115: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:37:05.731537: lr: 0.00982\n",
      "2023-01-04 18:37:05.735451: saving checkpoint...\n",
      "2023-01-04 18:37:05.760959: done, saving took 0.03 seconds\n",
      "2023-01-04 18:37:05.766804: This epoch took 37.942780 s\n",
      "\n",
      "2023-01-04 18:37:05.766948: \n",
      "epoch:  20\n",
      "2023-01-04 18:37:36.646073: train loss : -0.8385\n",
      "2023-01-04 18:37:42.792439: validation loss: -0.8563\n",
      "2023-01-04 18:37:42.792984: Average global foreground Dice: [0.9053, 0.8929]\n",
      "2023-01-04 18:37:42.793127: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:37:43.426461: lr: 0.009811\n",
      "2023-01-04 18:37:43.430469: saving checkpoint...\n",
      "2023-01-04 18:37:43.463151: done, saving took 0.04 seconds\n",
      "2023-01-04 18:37:43.468270: This epoch took 37.701218 s\n",
      "\n",
      "2023-01-04 18:37:43.468413: \n",
      "epoch:  21\n",
      "2023-01-04 18:38:14.166766: train loss : -0.8393\n",
      "2023-01-04 18:38:20.509731: validation loss: -0.8565\n",
      "2023-01-04 18:38:20.510290: Average global foreground Dice: [0.9056, 0.8922]\n",
      "2023-01-04 18:38:20.510452: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n",
      "2023-01-04 18:38:21.125723: lr: 0.009802\n",
      "2023-01-04 18:38:21.129745: saving checkpoint...\n",
      "2023-01-04 18:38:21.159028: done, saving took 0.03 seconds\n",
      "2023-01-04 18:38:21.163199: This epoch took 37.694679 s\n",
      "\n",
      "2023-01-04 18:38:21.163368: \n",
      "epoch:  22\n",
      "2023-01-04 18:38:52.419311: train loss : -0.8410\n",
      "2023-01-04 18:38:58.616896: validation loss: -0.8579\n",
      "2023-01-04 18:38:58.617428: Average global foreground Dice: [0.9058, 0.894]\n",
      "2023-01-04 18:38:58.617621: (interpret this as an estimate for the Dice of the different classes. This is not exact.)\n"
     ]
    }
   ],
   "source": [
    "!python -m plug_ai --config_file ../config_exemples/config_nnUNet.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e0ecc-7533-44cf-97f3-0d103475a7b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CLI help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97fa8162-f414-44e0-893c-c38bfabc0556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--dataset DATASET] [--dataset_kwargs DATASET_KWARGS]\n",
      "                   [--preprocess PREPROCESS]\n",
      "                   [--preprocess_kwargs PREPROCESS_KWARGS]\n",
      "                   [--batch_size BATCH_SIZE] [--train_ratio TRAIN_RATIO]\n",
      "                   [--val_ratio VAL_RATIO] [--limit_sample LIMIT_SAMPLE]\n",
      "                   [--shuffle SHUFFLE] [--drop_last DROP_LAST] [--model MODEL]\n",
      "                   [--model_kwargs MODEL_KWARGS] [--loop LOOP]\n",
      "                   [--loop_kwargs LOOP_KWARGS] [--nb_epoch NB_EPOCH]\n",
      "                   [--learning_rate LEARNING_RATE] [--device DEVICE]\n",
      "                   [--report_log REPORT_LOG] [--criterion CRITERION]\n",
      "                   [--criterion_kwargs CRITERION_KWARGS]\n",
      "                   [--optimizer OPTIMIZER]\n",
      "                   [--optimizer_kwargs OPTIMIZER_KWARGS]\n",
      "                   [--config_file CONFIG_FILE] [--export_config EXPORT_CONFIG]\n",
      "                   [--mode MODE] [--seed SEED] [--verbose VERBOSE]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "Data arguments:\n",
      "  --dataset DATASET     A dataset name in the valid list of of datasets\n",
      "                        supported by Plug_ai\n",
      "  --dataset_kwargs DATASET_KWARGS\n",
      "                        The dictionnary of args to use that are necessary for\n",
      "                        dataset\n",
      "  --preprocess PREPROCESS\n",
      "                        A valid preprocessing pipeline name provided by\n",
      "                        plug_ai\n",
      "  --preprocess_kwargs PREPROCESS_KWARGS\n",
      "                        A dictionnary of args that are given to the processing\n",
      "                        pipeline\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Number of samples to load per batch\n",
      "  --train_ratio TRAIN_RATIO\n",
      "                        Float : The fraction of the dataset to use for\n",
      "                        training, the rest will be used for final evaluation\n",
      "  --val_ratio VAL_RATIO\n",
      "                        Float : The fraction of the train set to use for\n",
      "                        validation (hp tuning)\n",
      "  --limit_sample LIMIT_SAMPLE\n",
      "                        Index value at which to stop when considering the\n",
      "                        dataset\n",
      "  --shuffle SHUFFLE     Boolean that indicates if the dataset should be\n",
      "                        shuffled at each epoch\n",
      "  --drop_last DROP_LAST\n",
      "                        Boolean that indicates if the last batch of an epoch\n",
      "                        should be left unused when incomplete.\n",
      "\n",
      "Model arguments:\n",
      "  --model MODEL         A model in the valid list of supported model or a\n",
      "                        callable that instantiate a Pytorch/Monai model\n",
      "  --model_kwargs MODEL_KWARGS\n",
      "                        Every arguments which should be passed to the model\n",
      "                        callable\n",
      "\n",
      "Execution arguments:\n",
      "  --loop LOOP\n",
      "  --loop_kwargs LOOP_KWARGS\n",
      "  --nb_epoch NB_EPOCH\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        Learning rate\n",
      "  --device DEVICE\n",
      "  --report_log REPORT_LOG\n",
      "  --criterion CRITERION\n",
      "  --criterion_kwargs CRITERION_KWARGS\n",
      "  --optimizer OPTIMIZER\n",
      "  --optimizer_kwargs OPTIMIZER_KWARGS\n",
      "\n",
      "Global arguments:\n",
      "  --config_file CONFIG_FILE\n",
      "                        Path : The config file to set parameters more easily\n",
      "  --export_config EXPORT_CONFIG\n",
      "                        Path : If given, save the full config(combining CLI\n",
      "                        and config file) at the given path\n",
      "  --mode MODE           String : A mode between \"TRAINING\", \"EVALUATION\" and\n",
      "                        \"INFERENCE\"\n",
      "  --seed SEED           Int : If given, sets random aspect by setting random\n",
      "                        numbers generators\n",
      "  --verbose VERBOSE     String or None: The level of verbose wanted. None,\n",
      "                        \"RESTRICTED\" or \"FULL\"\n"
     ]
    }
   ],
   "source": [
    "!python -m plug_ai -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaeebab-1c70-41e2-8fcc-4f9e2e0bfc54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test librairie (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db1ba2-9466-42d0-b920-e10041c78d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plug_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144c1a5-c624-4bda-843a-adbf6555e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plug_ai.data.dataset import get_dataset\n",
    "train_loader = get_dataset(args[\"dataset_dir\"], batch_size=args[\"batch_size\"], limit_sample=args[\"limit_sample\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3fe0f-9dc3-4380-a3d1-cf40b8c81f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plug_ai.data.dataset.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c912e38-3e6d-4c34-835b-42c833876f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utile pour recharger la librairie sans relancer le kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d2d20-6b8d-4193-98a9-4fad5817f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import inspect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
