{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cdf13b4-88bf-4443-b8f2-0eb74d65e0cb",
   "metadata": {},
   "source": [
    "Pour le moment, pour mener les tests comme si c'était un package installé, export path dans le .bashrc ou sys.path.append dans les script pour importer plug_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280eb6b-be5e-40d9-a95c-ce6c2a7a6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('/gpfswork/rech/ibu/ssos023/Plug-AI')\n",
    "#!export PYTHONPATH=$PYTHONPATH:/gpfsdswork/projects/rech/ibu/ssos023/Plug-AI/plug_ai\n",
    "#!export PYTHONPATH=$PYTHONPATH:/gpfsdswork/projects/rech/ibu/ssos023/Plug-AI/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c912e38-3e6d-4c34-835b-42c833876f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utile pour recharger la librairie sans relancer le kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d2d20-6b8d-4193-98a9-4fad5817f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf1ae1-9c79-4ecf-b6c6-72baec1ad039",
   "metadata": {},
   "source": [
    "        print(config)WARNING: There is an issue on frontal nodes. Kernel dies and there is a cuda initialization (from where? apparently dynunet from monai.networks.nets import DynUNet) that fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e812d-3ce9-4c08-9da7-2ae5a33dd622",
   "metadata": {},
   "source": [
    "### Test cells (to be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df34219-1a35-40b8-ab51-96735fb301ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe should we complete arguments list in cli automatically to include the dataset & model arguments\n",
    "# We can retrieve args (and types if defined) and add a fake init just like for managers to each dataset/model we define and have the manager inherit those args\n",
    "# For now, we add manually  (up to the user) those args in dataset_kwargs\n",
    "#monai.apps.DecathlonDataset.__init__.__code__.co_varnames\n",
    "#inspect.getfullargspec(monai.apps.DecathlonDataset.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786caee7-3ab0-43dc-aaa7-10552338eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnunet\n",
    "from nnunet.experiment_planning.nnUNet_plan_and_preprocess import main as test_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b722cd-95e5-43b1-a9fd-8e7bcfe89561",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export nnUNet_raw_data_base=\"/gpfswork/rech/ibu/commun/\"\n",
    "!export nnUNet_preprocessed=\"/gpfswork/rech/ibu/commun/nnUNet_preprocessed\"\n",
    "!export RESULTS_FOLDER=\"/gpfswork/rech/ibu/commun/nnUNet_trained_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2560d4-8fab-4c17-88bc-7c3d0135910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nnUNet_plan_and_preprocess -t 001 --verify_dataset_integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8af70-2cc2-4137-8f2a-c691d5a9f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#monai.apps.MedNISTDataset(root_dir = \"/gpfswork/rech/ibu/commun/datasets\", \n",
    "#                  section = \"training\",\n",
    "#                  download=download)\n",
    "MedNIST(dataset_dir = , download_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e04a8-e34c-4901-b787-448f0e92a0aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test CLI APP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cef5c5-d61c-46e2-a9aa-b48edb49dff0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CLI run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c0d9198-841b-4f02-87e4-dc0bfa0ed26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDRIS WARNING: can't determine process local rank for CUDA initialisation. No GPU available and/or no Slurm context defined.\n",
      "--------------------------------------------------------------------------\n",
      "The library attempted to open the following supporting CUDA libraries,\n",
      "but each of them failed.  CUDA-aware support is disabled.\n",
      "libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "libcuda.dylib: cannot open shared object file: No such file or directory\n",
      "/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory\n",
      "If you are not interested in CUDA-aware support, then run with\n",
      "--mca opal_warn_on_missing_libcuda 0 to suppress this message.  If you are interested\n",
      "in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location\n",
      "of libcuda.so.1 to get passed this issue.\n",
      "--------------------------------------------------------------------------\n",
      "Plug-AI running :\n",
      "==================================== Dataset initialization ... ====================================\n",
      "Preprocessing the dataset\n",
      "Dataset type is valid\n",
      "loading dataset...\n",
      "got datalist, extract: \n",
      " {'channel_0': '/gpfswork/rech/ibu/commun/BraTS2021/BraTS2021_Training_Data/BraTS2021_00081/BraTS2021_00081_flair.nii.gz', 'channel_1': '/gpfswork/rech/ibu/commun/BraTS2021/BraTS2021_Training_Data/BraTS2021_00081/BraTS2021_00081_t1ce.nii.gz', 'channel_2': '/gpfswork/rech/ibu/commun/BraTS2021/BraTS2021_Training_Data/BraTS2021_00081/BraTS2021_00081_t2.nii.gz', 'channel_3': '/gpfswork/rech/ibu/commun/BraTS2021/BraTS2021_Training_Data/BraTS2021_00081/BraTS2021_00081_t1.nii.gz', 'label': '/gpfswork/rech/ibu/commun/BraTS2021/BraTS2021_Training_Data/BraTS2021_00081/BraTS2021_00081_seg.nii.gz'}\n",
      "keys: ['channel_0', 'channel_1', 'channel_2', 'channel_3', 'label']\n",
      "Generating signature\n",
      "Generating signature for dataset\n",
      "===================================== Model initialization ... =====================================\n",
      "Model type is valid\n",
      "Model preparation done!\n",
      "verbose RESTRICTED\n",
      "=================================== Execution initialization ... ===================================\n",
      "TRAINING MODE : \n",
      "Training ...\n",
      "Criterion is : DiceCELoss(\n",
      "  (dice): DiceLoss()\n",
      "  (cross_entropy): CrossEntropyLoss()\n",
      ")\n",
      "Optimizer is : SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0.99\n",
      "    nesterov: True\n",
      "    weight_decay: 3e-05\n",
      ")\n",
      "start training loop, 500 steps per epoch\n",
      "Traceback (most recent call last):\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/pytorch-gpu-1.10.1+py3.9.7/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/gpfslocalsup/pub/anaconda-py3/2021.05/envs/pytorch-gpu-1.10.1+py3.9.7/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/gpfsdswork/projects/rech/ibu/ssos023/Plug-AI/plug_ai/__main__.py\", line 91, in <module>\n",
      "    main(config)\n",
      "  File \"/gpfsdswork/projects/rech/ibu/ssos023/Plug-AI/plug_ai/__main__.py\", line 57, in main\n",
      "    execution_manager = plug_ai.managers.ExecutionManager(dataset_manager = dataset_manager, \n",
      "  File \"/gpfsdswork/projects/rech/ibu/ssos023/Plug-AI/plug_ai/managers/managers.py\", line 458, in __init__\n",
      "    self.outputs = self.loop(**loop_kwargs_filtered)\n",
      "  File \"/gpfsdswork/projects/rech/ibu/ssos023/Plug-AI/plug_ai/runners/trainer.py\", line 55, in __init__\n",
      "    outputs = self.run()# model name, report log,... => train_loop kwargs\n",
      "  File \"/gpfsdswork/projects/rech/ibu/ssos023/Plug-AI/plug_ai/runners/trainer.py\", line 69, in run\n",
      "    loss = self.train_step(self, sample)\n",
      "TypeError: default_training_step() takes 2 positional arguments but 3 were given\n"
     ]
    }
   ],
   "source": [
    "!python -m plug_ai --export_config config_saved_test.yaml --dataset BraTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e0ecc-7533-44cf-97f3-0d103475a7b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CLI help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97fa8162-f414-44e0-893c-c38bfabc0556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDRIS WARNING: can't determine process local rank for CUDA initialisation. No GPU available and/or no Slurm context defined.\n",
      "--------------------------------------------------------------------------\n",
      "The library attempted to open the following supporting CUDA libraries,\n",
      "but each of them failed.  CUDA-aware support is disabled.\n",
      "libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "libcuda.dylib: cannot open shared object file: No such file or directory\n",
      "/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory\n",
      "If you are not interested in CUDA-aware support, then run with\n",
      "--mca opal_warn_on_missing_libcuda 0 to suppress this message.  If you are interested\n",
      "in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location\n",
      "of libcuda.so.1 to get passed this issue.\n",
      "--------------------------------------------------------------------------\n",
      "usage: __main__.py [-h] [--dataset DATASET] [--dataset_kwargs DATASET_KWARGS]\n",
      "                   [--preprocess PREPROCESS]\n",
      "                   [--preprocess_kwargs PREPROCESS_KWARGS]\n",
      "                   [--generate_signature GENERATE_SIGNATURE]\n",
      "                   [--batch_size BATCH_SIZE] [--limit_sample LIMIT_SAMPLE]\n",
      "                   [--shuffle SHUFFLE] [--drop_last DROP_LAST] [--model MODEL]\n",
      "                   [--checkpoints_path CHECKPOINTS_PATH]\n",
      "                   [--model_kwargs MODEL_KWARGS]\n",
      "                   [--use_signature USE_SIGNATURE] [--nb_epoch NB_EPOCH]\n",
      "                   [--learning_rate LEARNING_RATE] [--device DEVICE]\n",
      "                   [--report_log REPORT_LOG] [--criterion CRITERION]\n",
      "                   [--criterion_kwargs CRITERION_KWARGS]\n",
      "                   [--optimizer OPTIMIZER]\n",
      "                   [--optimizer_kwargs OPTIMIZER_KWARGS]\n",
      "                   [--execution_kwargs EXECUTION_KWARGS]\n",
      "                   [--config_file CONFIG_FILE] [--export_config EXPORT_CONFIG]\n",
      "                   [--mode MODE] [--seed SEED] [--verbose VERBOSE]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "Data arguments:\n",
      "  --dataset DATASET     A dataset name in the valid list of of datasets\n",
      "                        supported by plug_ai\n",
      "  --dataset_kwargs DATASET_KWARGS\n",
      "                        The dictionnary of args to use that are necessary for\n",
      "                        dataset\n",
      "  --preprocess PREPROCESS\n",
      "                        A valid preprocessing pipeline name provided by\n",
      "                        plug_ai OR your own preprocessing pipeline given as a\n",
      "                        collable\n",
      "  --preprocess_kwargs PREPROCESS_KWARGS\n",
      "                        A dictionnary of args that are given to the processing\n",
      "                        pipeline\n",
      "  --generate_signature GENERATE_SIGNATURE\n",
      "                        A boolean that indicates if nnUnet fingerprint should\n",
      "                        be determined.\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Number of samples to load per batch\n",
      "  --limit_sample LIMIT_SAMPLE\n",
      "                        Index value at which to stop when considering the\n",
      "                        dataset\n",
      "  --shuffle SHUFFLE     Boolean that indicates if the dataset should be\n",
      "                        shuffled at each epoch\n",
      "  --drop_last DROP_LAST\n",
      "                        Boolean that indicates if the last batch of an epoch\n",
      "                        should be left unused when incomplete.\n",
      "\n",
      "Model arguments:\n",
      "  --model MODEL\n",
      "  --checkpoints_path CHECKPOINTS_PATH\n",
      "  --model_kwargs MODEL_KWARGS\n",
      "  --use_signature USE_SIGNATURE\n",
      "\n",
      "Execution arguments:\n",
      "  --nb_epoch NB_EPOCH\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        Learning rate\n",
      "  --device DEVICE\n",
      "  --report_log REPORT_LOG\n",
      "  --criterion CRITERION\n",
      "  --criterion_kwargs CRITERION_KWARGS\n",
      "  --optimizer OPTIMIZER\n",
      "  --optimizer_kwargs OPTIMIZER_KWARGS\n",
      "  --execution_kwargs EXECUTION_KWARGS\n",
      "\n",
      "Global arguments:\n",
      "  --config_file CONFIG_FILE\n",
      "  --export_config EXPORT_CONFIG\n",
      "                        test\n",
      "  --mode MODE\n",
      "  --seed SEED\n",
      "  --verbose VERBOSE\n"
     ]
    }
   ],
   "source": [
    "!python -m plug_ai -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaeebab-1c70-41e2-8fcc-4f9e2e0bfc54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test librairie (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db1ba2-9466-42d0-b920-e10041c78d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plug_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144c1a5-c624-4bda-843a-adbf6555e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plug_ai.data.dataset import get_dataset\n",
    "train_loader = get_dataset(args[\"dataset_dir\"], batch_size=args[\"batch_size\"], limit_sample=args[\"limit_sample\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3fe0f-9dc3-4380-a3d1-cf40b8c81f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plug_ai.data.dataset.datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
